---
title: "Project 4"
author: "Susanna Wong"
date: "2023-04-30"
output:
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project 4: Document Classification
It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  
For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:   https://spamassassin.apache.org/old/publiccorpus/ 

# Dataset from Kaggle {.tabset}

The dataset is obtained from Kaggle. It contains 5171 observations of spam and ham. 

## Download 

The csv file is uploaded into my github. 
Use `read.csv` to read in the csv file. 
```{r}
kaggle <- read.csv("https://raw.githubusercontent.com/suswong/DATA-607-Project-4/main/spam_ham_dataset.csv")
kaggle <- kaggle[,-4]
```

## Number of Spam and Ham
There are 3672 spam and 1499 spam. 
```{r}
library("dplyr")
library("DT")
ham_or_spam <- kaggle%>%
  count(label)
datatable(ham_or_spam)
```

# Create a Corpus and Document Term Matrix
To clean the corpus, we need to make all text into lowercase, remove numbers, remove stopwords, remove punctucation, and remove whitespaces.

```{r}

#install.packages("tm")
library(tm)
spam_ham_corpus <- Corpus(VectorSource(as.vector(kaggle$text)))
spam_ham_corpus <- tm_map(spam_ham_corpus, content_transformer(tolower))
spam_ham_corpus <- tm_map(spam_ham_corpus, removeNumbers)
spam_ham_corpus <- tm_map(spam_ham_corpus, removeWords, stopwords("english"))
spam_ham_corpus <- tm_map(spam_ham_corpus, removePunctuation)
spam_ham_corpus <- tm_map(spam_ham_corpus, stripWhitespace)
spam_ham_corpus <- tm_map(spam_ham_corpus, stemDocument)

DTM <- DocumentTermMatrix(spam_ham_corpus)

# Remove sparse terms with threshold of 90%
DTM <- removeSparseTerms(DTM, 0.90)
spam_ham <- as.data.frame(as.matrix(DTM))
spam_ham$label <- kaggle$label
```

# Training and Testing

70% of the observation will be used to train and 30% for testing.
```{r}
#install.packages("caTools")
library(caTools)
set.seed(1111)

sample <- sample.split(kaggle$text, SplitRatio = 0.8)
train  <- subset(kaggle, sample == TRUE)
test   <- subset(kaggle, sample == FALSE)
``` 

## Count number of spam and ham in each set 

```{r}
train_count <- train%>%
  count(label)
datatable(train_count)
```
```{r}
test_count <- test%>%
  count(label)
datatable(test_count)
```
## Model and Prediction

```{r}
#install.packages("naivebayes")
#install.packages("e1071")

library(naivebayes)
library(e1071)
model <- naive_bayes(label~., train)

prediction <- predict(model, test)

```
# Model Accuracy
```{r}
#install.packages("caret")
library(caret)
confusion_matrix <- table(test$label, prediction)

confusionMatrix(confusion_matrix)

```
# Conclusion

The model above has a 75.17% accuracy rate, using Naive Bayes to train and predict. 

# Source
http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know

https://www.statology.org/train-test-split-r/
